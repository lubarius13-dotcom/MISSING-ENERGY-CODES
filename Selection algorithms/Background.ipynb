{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa5aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Predefine sets as NumPy arrays for fast isin\n",
    "VISIBLE_PDGS = np.array([\n",
    "    211, -211,    # pi±\n",
    "    111,          # pi0\n",
    "    321, -321,    # K±\n",
    "    2212, -2212,  # p, p̅\n",
    "    2112, -2112,  # n, n̅\n",
    "    130,          # K_L0\n",
    "    22,           # γ\n",
    "    11, -11,      # e±\n",
    "    13, -13       # μ±\n",
    "], dtype=int)\n",
    "\n",
    "CHARGED_PDGS = np.array([\n",
    "    211, -211,\n",
    "    321, -321,\n",
    "    2212, -2212,\n",
    "    11, -11,\n",
    "    13, -13\n",
    "], dtype=int)\n",
    "\n",
    "LEPTON_PDGS = {11, -11, 13, -13}  # e±, μ±\n",
    "\n",
    "\n",
    "def compute_thrust_event_aleph_like(px, py, pz, n_iter=10, n_random=5):\n",
    "    \"\"\"\n",
    "    ALEPH-like thrust finder:\n",
    "    - Uses |p|, |pz|, and a few random seeds\n",
    "    - Less 'perfect' than all-seeds, closer to ALEPH reconstruction\n",
    "    \"\"\"\n",
    "    p = np.column_stack((px, py, pz))\n",
    "    p_mag = np.linalg.norm(p, axis=1)\n",
    "    total_mag = np.sum(p_mag)\n",
    "\n",
    "    if total_mag == 0:\n",
    "        return 0.0, 1.0, np.array([0., 0., 1.])\n",
    "\n",
    "    seeds = []\n",
    "\n",
    "    # 1. Seed from largest |p|\n",
    "    idx_maxp = np.argmax(p_mag)\n",
    "    seeds.append(p[idx_maxp] / p_mag[idx_maxp])\n",
    "\n",
    "    # 2. Seed from largest |pz|\n",
    "    idx_maxpz = np.argmax(np.abs(p[:, 2]))\n",
    "    if p_mag[idx_maxpz] > 0:\n",
    "        seeds.append(p[idx_maxpz] / p_mag[idx_maxpz])\n",
    "\n",
    "    # 3. Random seeds from visible particles\n",
    "    n_available = len(p)\n",
    "    if n_available > 0:\n",
    "        n_rand = min(n_random, n_available)\n",
    "        rand_idx = np.random.choice(n_available, n_rand, replace=False)\n",
    "        for i in rand_idx:\n",
    "            if p_mag[i] > 0:\n",
    "                seeds.append(p[i] / p_mag[i])\n",
    "\n",
    "    best_thrust = -1.0\n",
    "    best_axis = np.array([0., 0., 1.])\n",
    "\n",
    "    # Iterate thrust maximization\n",
    "    for axis in seeds:\n",
    "        prev_thrust = -1.0\n",
    "        for _ in range(n_iter):\n",
    "            dots = p @ axis\n",
    "            signs = np.sign(dots)\n",
    "            new_axis = np.sum(signs[:, None] * p, axis=0)\n",
    "            norm = np.linalg.norm(new_axis)\n",
    "            if norm == 0:\n",
    "                break\n",
    "            axis = new_axis / norm\n",
    "            thrust_val = np.sum(np.abs(p @ axis)) / total_mag\n",
    "            if abs(thrust_val - prev_thrust) < 1e-6:\n",
    "                break\n",
    "            prev_thrust = thrust_val\n",
    "\n",
    "        # Final thrust\n",
    "        thrust_val = np.sum(np.abs(p @ axis)) / total_mag\n",
    "        if thrust_val > best_thrust:\n",
    "            best_thrust = thrust_val\n",
    "            best_axis = axis\n",
    "\n",
    "    cos_theta_thrust = abs(best_axis[2])\n",
    "    return best_thrust, cos_theta_thrust, best_axis\n",
    "\n",
    "def compute_thrust_with_angle2(px, py, pz, n_iter=30, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Iterative thrust with |p|-seed, early convergence, and sign-robust stopping.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    thrust_val : float\n",
    "    cos_theta_thrust : float\n",
    "    axis : np.ndarray shape (3,)\n",
    "    \"\"\"\n",
    "    p = np.column_stack((px, py, pz)).astype(float)\n",
    "    p_mag = np.linalg.norm(p, axis=1)\n",
    "    denom = p_mag.sum()\n",
    "    if denom == 0.0:\n",
    "        return 0.0, 1.0, np.array([0.0, 0.0, 1.0])\n",
    "\n",
    "    # Seed with direction of largest |p|\n",
    "    lead = np.argmax(p_mag)\n",
    "    axis = p[lead]\n",
    "    norm = np.linalg.norm(axis)\n",
    "    axis = axis / norm if norm > 0.0 else np.array([0.0, 0.0, 1.0])\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        # s_i = sign(p_i · n); resolve exact zeros deterministically as +1\n",
    "        proj = p @ axis\n",
    "        signs = np.sign(proj)\n",
    "        signs[signs == 0.0] = 1.0\n",
    "\n",
    "        new_axis = (signs[:, None] * p).sum(axis=0)\n",
    "        new_norm = np.linalg.norm(new_axis)\n",
    "        if new_norm == 0.0:\n",
    "            break\n",
    "        new_axis /= new_norm\n",
    "\n",
    "        # Convergence up to a global sign: stop when n and new_n are aligned\n",
    "        if 1.0 - abs(np.dot(axis, new_axis)) < tol:\n",
    "            axis = new_axis\n",
    "            break\n",
    "\n",
    "        axis = new_axis\n",
    "\n",
    "    thrust_val = np.sum(np.abs(p @ axis)) / denom\n",
    "    cos_theta_thrust = abs(axis[2])\n",
    "    return thrust_val, cos_theta_thrust, axis\n",
    "\n",
    "\n",
    "def has_identified_lepton(pdg_h, E_h, p_h, p_min=2.0):\n",
    "    \"\"\"\n",
    "    Proxy for ALEPH lepton ID: hemisphere contains an electron or muon \n",
    "    with momentum > p_min GeV.\n",
    "    \"\"\"\n",
    "    for pdg, E, p in zip(pdg_h, E_h, p_h):\n",
    "        if abs(pdg) in LEPTON_PDGS and p > p_min:\n",
    "            return True\n",
    "    return False\n",
    "  \n",
    "def missingE_with_thrust_and_lepton_veto(\n",
    "    input_path,\n",
    "    output_path,\n",
    "    E_beam=45.6,\n",
    "    progress_step=100_000\n",
    "):\n",
    "    \"\"\"\n",
    "    Loop over all events and store hemisphere missing energies\n",
    "    using the thrust axis (ALEPH style), with thrust cuts and lepton veto\n",
    "    \"\"\"\n",
    "    \n",
    "    scanned = 0\n",
    "    kept = 0\n",
    "    \n",
    "    cut_counts = {\n",
    "        \"thrust_cos\": 0,\n",
    "        \"lep_veto\": 0,\n",
    "    }\n",
    "\n",
    "    with open(input_path, \"r\") as fin, \\\n",
    "         open(output_path, \"w\", newline=\"\") as fout:\n",
    "\n",
    "        writer = csv.writer(fout)\n",
    "        writer.writerow([\"E_miss\", \"hemisphere\", \"event_id\"])\n",
    "\n",
    "        for ev_id, raw in enumerate(fin):\n",
    "            line = raw.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            tk = line.split()\n",
    "            if len(tk) % 9 != 0:\n",
    "                raise ValueError(f\"Line has {len(tk)} tokens (not multiple of 9).\")\n",
    "\n",
    "            arr = np.fromiter((float(x) for x in tk), dtype=float).reshape(-1, 9)\n",
    "            pdg = arr[:, 0].astype(int)\n",
    "            px, py, pz, E = arr[:, 1:5].T\n",
    "            x_prod, y_prod, z_prod, r_prod = arr[:, 5:9].T\n",
    "            \n",
    "            scanned += 1\n",
    "            if scanned % progress_step == 0:\n",
    "                print(f\"Processed {scanned:,} events...\")\n",
    "\n",
    "             # --- Build visible mask (exclude neutrinos etc.) ---\n",
    "            vis_mask = np.isin(pdg, VISIBLE_PDGS, assume_unique=True)\n",
    "\n",
    "            # --- Thrust cuts ---\n",
    "            thrust, cos_theta, axis = compute_thrust_with_angle2(px[vis_mask], py[vis_mask], pz[vis_mask])\n",
    "            if thrust <= 0.85 or cos_theta >= 0.7:\n",
    "                continue\n",
    "            cut_counts[\"thrust_cos\"] += 1\n",
    "\n",
    "            # Split into hemispheres wrt thrust axis\n",
    "            dot_products = px * axis[0] + py * axis[1] + pz * axis[2]\n",
    "            mask_forward = dot_products > 0\n",
    "            hemispheres = (mask_forward, ~mask_forward)\n",
    "\n",
    "            # --- Lepton veto: check both hemispheres ---\n",
    "            veto = False\n",
    "            for hmask in hemispheres:\n",
    "                pdg_h = pdg[hmask]\n",
    "                E_h   = E[hmask]\n",
    "                p_h   = np.sqrt(px[hmask]**2 + py[hmask]**2 + pz[hmask]**2)\n",
    "                if has_identified_lepton(pdg_h, E_h, p_h):\n",
    "                    veto = True\n",
    "                    break\n",
    "            if veto:\n",
    "                continue\n",
    "            cut_counts[\"lep_veto\"] += 1\n",
    "\n",
    "            # --- Compute missing energy per hemisphere ---\n",
    "            for h_idx, hmask in enumerate(hemispheres):\n",
    "                pdg_h = pdg[hmask]\n",
    "                E_h   = E[hmask]\n",
    "\n",
    "                vis_mask = np.isin(pdg_h, VISIBLE_PDGS, assume_unique=True)\n",
    "                E_vis_h = np.sum(E_h[vis_mask])\n",
    "                E_miss_h = E_beam - E_vis_h\n",
    "\n",
    "                writer.writerow([E_miss_h, h_idx, ev_id])\n",
    "\n",
    "            kept += 1\n",
    "\n",
    "    print(f\"\\nScanned {scanned:,} events, stored {kept:,} hemispheres into {output_path}\")\n",
    "    print(\"Cutflow:\", cut_counts)\n",
    "    return kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf00b6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100,000 events...\n",
      "Processed 200,000 events...\n",
      "Processed 300,000 events...\n",
      "Processed 400,000 events...\n",
      "Processed 500,000 events...\n",
      "Processed 600,000 events...\n",
      "Processed 700,000 events...\n",
      "Processed 800,000 events...\n",
      "Processed 900,000 events...\n",
      "Processed 1,000,000 events...\n",
      "Processed 1,100,000 events...\n",
      "Processed 1,200,000 events...\n",
      "Processed 1,500,000 events...\n",
      "Processed 1,600,000 events...\n",
      "Processed 1,700,000 events...\n",
      "Processed 1,800,000 events...\n",
      "Processed 1,900,000 events...\n",
      "Processed 2,000,000 events...\n",
      "Processed 2,100,000 events...\n",
      "Processed 2,200,000 events...\n",
      "Processed 2,300,000 events...\n",
      "Processed 2,400,000 events...\n",
      "Processed 2,500,000 events...\n",
      "Processed 2,600,000 events...\n",
      "Processed 2,700,000 events...\n",
      "Processed 2,800,000 events...\n",
      "Processed 2,900,000 events...\n",
      "Processed 3,000,000 events...\n",
      "Processed 3,100,000 events...\n",
      "Processed 3,200,000 events...\n",
      "Processed 3,300,000 events...\n",
      "Processed 3,400,000 events...\n",
      "Processed 3,500,000 events...\n",
      "Processed 3,600,000 events...\n",
      "Processed 3,700,000 events...\n",
      "Processed 3,800,000 events...\n",
      "Processed 3,900,000 events...\n",
      "Processed 4,000,000 events...\n",
      "Processed 4,100,000 events...\n",
      "\n",
      "Scanned 4,176,778 events, stored 1,888,366 hemispheres into background_smeared.csv\n",
      "Cutflow: {'thrust_cos': 2286876, 'lep_veto': 1888366}\n"
     ]
    }
   ],
   "source": [
    "filename = \"selected_hadronic_smeared.txt\"\n",
    "kept = missingE_with_thrust_and_lepton_veto(\n",
    "    filename,\n",
    "    \"background_smeared.csv\",\n",
    "    E_beam=45.6,\n",
    "    progress_step=100_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cc204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
